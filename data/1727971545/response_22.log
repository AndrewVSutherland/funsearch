Here's an improved version of `priority_v1` that considers the number of unique remainders when dividing `k` by `n`. This is a small change that builds upon `priority_v0` and should be easy to understand and implement.

```python
def priority_v1(k: int, n: int) -> float:
  """Improved version of `priority_v0`."""
  return len(set((k + i) % n for i in range(n))) / n
```

This version calculates the priority as the ratio of unique remainders to `n`. The set of remainders is obtained by iterating over `i` from 0 to `n-1` and calculating `(k + i) % n`. The `set()` function removes duplicates, and the length of the set is divided by `n` to normalize the result between 0 and 1.

To further improve the priority function, you could consider other factors, such as the distribution of remainders or the number of times each remainder appears. Here's a slightly more complex version that considers these factors:

```python
def priority_v2(k: int, n: int) -> float:
  """Further improved version of `priority_v1`."""
  remainders = [(k + i) % n for i in range(n)]
  unique_remainders = set(remainders)
  return (len(unique_remainders) / n) * (sum(n - np.bincount(list(unique_remainders)).astype(int)) / (n * (n - 1)))
```

In this version, we first calculate the list of remainders and then find the unique remainders using a set. The priority is then calculated as the product of two terms:

1. The ratio of unique remainders to `n` (as in `priority_v1`).
2. The average number of times each unique remainder appears, normalized between 0 and 1. This term encourages remainders that appear fewer times, as they are more likely to be part of a Salem-Spencer set.

You can continue to improve the priority function by adding more factors or using more complex heuristics. However, keep in mind that the goal is to make small changes and keep the code short, as stated in the original problem description.