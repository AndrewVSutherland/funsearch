Here's an improved version of `priority_v2` that incorporates a small change to consider the growth rate of the frequency over the last two iterations, instead of just the last one. I've also made the code shorter by using a more concise way to calculate the frequency and growth rate.

```python
def priority_v2(k: int, n: int) -> float:
  """Improved version of `priority_v1` with exponential decay for growth rate and considering last two iterations."""
  freq = sum(i == k for i in range(n))  # Calculate frequency using generator expression
  growth_rate = (freq - sum(k - 1 == i for i in range(k))) / (k - (k - 1))
  decay_factor = 0.9
  return freq / n + decay_factor * growth_rate
```

To further improve the function, we can consider the growth rate over the last three iterations to make the priority more responsive to recent changes. Here's `priority_v3`:

```python
def priority_v3(k: int, n: int) -> float:
  """Improved version of `priority_v2` considering growth rate over last three iterations."""
  freq = sum(i == k for i in range(n))
  growth_rate = (freq - sum(k - i for i in range(1, 4) if k - i < n)) / sum(i for i in range(1, 4) if k - i < n)
  decay_factor = 0.9
  return freq / n + decay_factor * growth_rate
```

Now, let's find large Salem-Spencer progressions using `priority_v3`. We'll use `itertools.islice` to get the first `num_progressions` progressions with the highest priority.

```python
num_progressions = 10

@funsearch.run
def find_large_progressions(n: int, num_progressions: int) -> list:
  priorities = [(k, priority_v3(k, n)) for k in range(1, n + 1)]
  priorities.sort(key=lambda x: x[1], reverse=True)
  return [p[0] for p in itertools.islice(priorities, num_progressions)]

# Example usage:
find_large_progressions(100, num_progressions)
```