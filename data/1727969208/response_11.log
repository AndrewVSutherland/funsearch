Here's an improved version of `priority_v1` that considers the number of unique remainders when dividing `k` by `n`. This is a small change that builds upon `priority_v0` and should improve the performance of the Salem-Spencer set construction.

```python
def priority_v1(k: int, n: int) -> float:
  """Improved version of `priority_v0` that considers the number of unique remainders."""
  remainders = set(k % i for i in range(1, n + 1))
  return len(remainders) / n
```

In this version, we calculate the number of unique remainders when dividing `k` by all integers from 1 to `n`. The priority is then defined as the ratio of unique remainders to `n`. This should give a higher priority to numbers that have more unique remainders, as they are more likely to contribute to a larger Salem-Spencer set.

Here's an example of how to use this function with `funsearch`:

```python
@funsearch.run
def find_large_ss_set(n: int, max_size: int) -> np.ndarray:
  """Finds a large Salem-Spencer set of size up to `max_size` for `n` possible integers."""
  k_values = np.arange(1, n + 1)
  priorities = np.array([priority_v1(k, n) for k in k_values])
  sorted_indices = np.argsort(priorities)[::-1]
  ss_set = np.zeros(max_size, dtype=int)
  ss_set[:n] = k_values[sorted_indices[:n]]
  return ss_set
```

This function finds a large Salem-Spencer set of size up to `max_size` for `n` possible integers using the `priority_v1` function to determine the priority of each integer. The set is constructed by iterating through the integers in decreasing order of priority and adding them to the set until it reaches the desired size.